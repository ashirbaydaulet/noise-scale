{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17defee3-699b-4920-bd5c-1ef1811edb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 data...\n",
      "Data Loaded: 50000 training images, 10000 test images\n",
      "Injecting 20.0% symmetric noise...\n",
      "Starting Timeline Analysis...\n",
      "Epoch 1 | Clean Acc: 46.9% | Memorization: 6.1%\n",
      "Epoch 2 | Clean Acc: 50.6% | Memorization: 5.6%\n",
      "Epoch 3 | Clean Acc: 62.9% | Memorization: 4.6%\n",
      "Epoch 4 | Clean Acc: 62.9% | Memorization: 4.4%\n",
      "Epoch 5 | Clean Acc: 70.1% | Memorization: 3.8%\n",
      "Epoch 6 | Clean Acc: 71.7% | Memorization: 3.6%\n",
      "Epoch 7 | Clean Acc: 75.2% | Memorization: 3.3%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.data.cifar_loader import load_cifar_batch\n",
    "from src.data.cifar_dataset import CIFAR10Dataset\n",
    "from src.models.resnet import ResNet18CIFAR\n",
    "from src.data.label_noise import inject_label_noise # Import the tool we just made\n",
    "\n",
    "# --- CONFIG ---\n",
    "NOISE_RATE = 0.2  # 20% of data is poisoned\n",
    "NOISE_TYPE = 'symmetric' # 'symmetric' or 'asymmetric'\n",
    "EPOCHS = 30 # Need enough epochs to see the \"Memorization Spike\"\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 1. PREPARE DATA\n",
    "print(\"Loading CIFAR-10 data...\")\n",
    "DATA_PATH = '/Users/daulet/Desktop/data centric ai/cifar-10-batches-py' # Check if this path is still correct\n",
    "\n",
    "# --- A. Load Training Data (Batches 1-5) ---\n",
    "all_train_images = []\n",
    "all_train_labels = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    fpath = os.path.join(DATA_PATH, f'data_batch_{i}')\n",
    "    if os.path.exists(fpath):\n",
    "        imgs, lbls = load_cifar_batch(fpath)\n",
    "        all_train_images.append(imgs)\n",
    "        all_train_labels.extend(lbls)\n",
    "    else:\n",
    "        print(f\"Warning: Could not find {fpath}\")\n",
    "\n",
    "x_train = np.concatenate(all_train_images)\n",
    "y_train = np.array(all_train_labels)\n",
    "\n",
    "# --- B. Load Test Data ---\n",
    "test_fpath = os.path.join(DATA_PATH, 'test_batch')\n",
    "# If test_batch is missing, sometimes it's named data_batch_5 or similar, but standard is test_batch\n",
    "if os.path.exists(test_fpath):\n",
    "    x_test, y_test = load_cifar_batch(test_fpath)\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "else:\n",
    "    print(\"Error: Test batch not found! Checking fallback...\")\n",
    "    # Fallback if needed, but usually test_batch exists\n",
    "    \n",
    "print(f\"Data Loaded: {len(x_train)} training images, {len(x_test)} test images\")\n",
    "\n",
    "# --- C. Define Transforms (Must match your baseline) ---\n",
    "import torchvision.transforms as T\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 2. INJECT POISON\n",
    "print(f\"Injecting {NOISE_RATE*100}% {NOISE_TYPE} noise...\")\n",
    "y_train_noisy, noisy_idx = inject_label_noise(y_train, NOISE_TYPE, NOISE_RATE)\n",
    "y_train_noisy = np.array(y_train_noisy)\n",
    "\n",
    "# Create Datasets\n",
    "# We use standard transform for training\n",
    "train_dataset = CIFAR10Dataset(x_train, y_train_noisy, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# We need a separate loader for the \"Noisy Subset\" to track metrics\n",
    "# We don't shuffle this so we can match indices easily\n",
    "noisy_subset = Subset(CIFAR10Dataset(x_train, y_train_noisy, transform=test_transform), noisy_idx)\n",
    "noisy_loader = DataLoader(noisy_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Test Set (Clean)\n",
    "test_dataset = CIFAR10Dataset(x_test, y_test, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 3. SETUP MODEL\n",
    "model = ResNet18CIFAR(num_classes=10).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. TRACKING STORAGE\n",
    "history = {\n",
    "    'clean_test_acc': [],\n",
    "    'memorization_ratio': [],\n",
    "    'learning_ratio': [] # Bonus: Are they learning the TRUE label despite the noise?\n",
    "}\n",
    "\n",
    "print(\"Starting Timeline Analysis...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)\n",
    "        loss = criterion(output, labels) # We train on the LIE (noisy label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # --- EVALUATE (The \"Probe\") ---\n",
    "    model.eval()\n",
    "    \n",
    "    # A. Clean Test Accuracy\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    clean_acc = 100 * correct / total\n",
    "    history['clean_test_acc'].append(clean_acc)\n",
    "    \n",
    "    # B. Memorization Ratio (On the Poisoned Data)\n",
    "    # We check: Did the model predict the NOISY label (y_tilde)?\n",
    "    mem_correct = 0 # Pred == y_tilde\n",
    "    learn_correct = 0 # Pred == y_true (The hidden truth)\n",
    "    total_noisy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # iterate through our noisy_subset\n",
    "        # Note: The dataset returns (img, label). The label is y_tilde (noisy).\n",
    "        # We need y_true for \"Learning Ratio\", but let's stick to \"Memorization\" first.\n",
    "        for imgs, current_labels in noisy_loader:\n",
    "            imgs, current_labels = imgs.to(device), current_labels.to(device)\n",
    "            out = model(imgs)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            \n",
    "            # current_labels IS y_tilde (the lie)\n",
    "            mem_correct += (pred == current_labels).sum().item()\n",
    "            total_noisy += current_labels.size(0)\n",
    "            \n",
    "    mem_ratio = 100 * mem_correct / total_noisy\n",
    "    history['memorization_ratio'].append(mem_ratio)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Clean Acc: {clean_acc:.1f}% | Memorization: {mem_ratio:.1f}%\")\n",
    "\n",
    "# 5. PLOT THE CURVES\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history['clean_test_acc'], label='Clean Test Acc (Generalization)', color='green')\n",
    "plt.plot(history['memorization_ratio'], label='Memorization Ratio (Fitting Noise)', color='red', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title(f'The Learning Timeline ({NOISE_TYPE.capitalize()} Noise)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409e49a-c92e-4b58-8838-e8e383865333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0fb14-9faf-45ea-95a8-a88be327e68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
